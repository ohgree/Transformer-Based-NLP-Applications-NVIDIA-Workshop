{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec34iPLEhMFg"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTgZfeuKhMFl"
   },
   "source": [
    "# 평가: 저자 판별\n",
    "### (NVIDIA NeMo v1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A9IqGBUhMFl"
   },
   "source": [
    "저작권 표시는 일종의 텍스트 분류 문제입니다.  질병 텍스트 분류 문제에서 그랬던 것처럼 텍스트를 _주제_별로 분류하는 대신 텍스트를 _저자_별로 분류하는 것이 목적입니다.  \n",
    "\n",
    "이와 같은 문제를 해결하려고 시도하는 데 있어서 본질적인 가정은 해당 저자들의 *문체에 몇 가지 차이*가 있으며 *모델이 이러한 차이를 알아차릴 수 있다는 것입니다*.  이것이 BERT 등에도 해당될까요?  언어 모델이 문체를 \"이해\"할 수 있을까요? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbHgjpJDhMFm"
   },
   "source": [
    "### 목차\n",
    "[문제](#문제)<br>\n",
    "[채점](#채점)<br>\n",
    "[1단계: 데이터 준비](#1단계:-데이터-준비)<br>\n",
    "[2단계: 모델 Configuration 준비](#2단계:-모델-Configuration-준비)<br>\n",
    "[3단계: Trainer Configuration 준비](#3단계:-Trainer-Configuration-준비)<br>\n",
    "[4단계: 트레이닝](#4단계:-트레이닝)<br>\n",
    "[5단계: 추론](#5단계:-추론)<br>\n",
    "[6단계: 평가 제출](#6단계:-평가-제출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8qQODlahMFn"
   },
   "source": [
    "# 문제\n",
    "### 연방주의자 논집 - 역사 미스터리!\n",
    "\n",
    "[연방주의자 논집](https://en.wikipedia.org/wiki/The_Federalist_Papers)은 1787년과 1788년 사이에 [Alexander Hamilton](https://en.wikipedia.org/wiki/Alexander_Hamilton), [James Madison](https://en.wikipedia.org/wiki/James_Madison), [John Jay](https://en.wikipedia.org/wiki/John_Jay)가 작성한 에세이집입니다.  처음에 'Publius'라는 필명으로 발행되었으며 저자들의 의도는 당시 새로운 미국 헌법의 비준을 독려하는 것이었습니다.  후년에 가서 85개 논문의 각 저자가 확인된 목록이 알려졌습니다.  그럼에도 불구하고 이러한 논문의 하위 세트는 저자가 여전히 알려져 있지 않습니다.  연방주의자 논집 저자 판별 문제는 과거에 진행된 대부분의 통계 NLP 연구의 주제였습니다.   이제 자체 BERT 기반 프로젝트 모델을 사용하여 이 문제를 해결해 보겠습니다.\n",
    "<img style=\"float: right;\" src=\"images/HandM.png\" width=400>\n",
    "                                                                                                           \n",
    "구체적으로 말해서, 문제는 논쟁이 되고 있는 각 논문의 저자가 Alexander Hamilton인지, 아니면 James Madison인지 알아내는 것입니다.  이 연습에서는 각 논문의 저자가 한 명이며(즉, *100%* 확실하지는 않지만 공저가 이루어지지 않았음) 각 저자는 확인된 모든 논문 전체에서 보여진 잘 정의된 문체를 지지고 있다고 가정할 수 있습니다. \n",
    "\n",
    "### 프로젝트\n",
    "이 프로젝트를 위해 레이블이 지정된 `train.tsv` 및 `dev.tsv` 데이터세트가 제공됩니다.  논쟁이 되고 있는 각 논문에 하나씩, 총 10개의 테스트 세트가 있습니다.  모든 데이터세트는 `data/federalist_papers_HM` 디렉토리에 포함되어 있습니다.  \n",
    "\n",
    "각 \"문장\"은 사실상 약 256개의 단어로 구성된 문장의 그룹입니다.  레이블은 HAMILTON의 경우 '0', MADISON의 경우 '1'입니다.  예제 파일에는 Madison의 논문보다 Hamilton의 논문이 더 많습니다.  대략적으로 동일한 분포의 레이블 2개를 트레이닝 세트로 사용해 검증 세트를 만들었습니다.\n",
    "\n",
    "여러분이 해야 할 작업은 랩 2에서 했던 것처럼 NeMo를 사용해 뉴럴 네트워크를 구축하는 것입니다.  모델을 트레이닝하고 테스트하게 됩니다.  그런 다음 제공된 대조 코드를 사용해 모델이 이 \"역사 미스터리\"에 제공하는 답변을 확인해 보십시오! \n",
    "\n",
    "이 과정에서 코드 스니펫을 저장한 다음 오토그레이더(autograder)를 사용해 테스트합니다.  이 부분은 노트북 끝의 제출 지침을 따르십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_wFNvpDhMFo"
   },
   "source": [
    "---\n",
    "# 채점\n",
    "최종 결과가 아닌 프로젝트에 맞게 모델을 설정하고 트레이닝하는 능력을 평가받게 됩니다.  이 코딩 평가는 70점 만점이며, 다음과 같이 나뉘어집니다.\n",
    "\n",
    "### 지시문\n",
    "\n",
    "| 단계                                | 평가                                                    | FIXMEs?  | 점수 |\n",
    "|--------------------------------------|-----------------------------------------------------------|----------|--------|\n",
    "| 1. 프로젝트 준비               | 데이터 형식 수정 (정확한 형식)                          |  2       | 10     |\n",
    "| 2. 모델 Configuration 준비   | 재정의를 위한 모델 파라미터 설정                         |  3       | 15     |\n",
    "| 3. Trainer Configuration 준비 | 재정의를 위한 trainer parameters 설정                       |  3       | 15     |\n",
    "| 4. 트레이닝                            | 트레이너 실행(트레이닝 로그가 트레이닝이 올바름을 나타냄) |  4       | 20      |\n",
    "| 5. 추론                             | 추론 실행(결과가 유효한 프로젝트임을 나타냄)          |  0       | 10     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H95O-4nhMFo"
   },
   "source": [
    "이 시점에서 여러분은 도움을 전혀 받지 않고도 프로젝트를 매우 능숙하게 구축할 수 있지만 특정한 변수 이름을 포함해 몇 가지 리소스가 주어집니다.  이는 오토그레이더(autograder)를 위한 것이므로 평가에 이러한 구조를 사용하십시오.  또한, 이번 평가는 커맨트 라인 메소드를 테스트하고 `text_classification_with_bert.py` 스크립트와 커맨드라인 트레이닝 메소드의 사용을 가정하고 있습니다. 모델 이름, 시퀀스 길이, 배치 크기, 학습률, 에포크 수 등과 같은 매개변수를 자유롭게 변경하여 모델을 적합하게 개선할 수 있습니다. \n",
    "\n",
    "신뢰할 수 있는 모델을 구축했다는 확신이 들면 노트북 끝의 제출 지침을 따르십시오.\n",
    "\n",
    "### 리소스 및 힌트\n",
    "* **예제 코드:**<br>\n",
    "왼쪽의 파일 탐색기에서 `lab2_reference_notebooks` 디렉토리를 찾습니다.  여기에는 예제로 사용할 NER 및 텍스트 분류를 위한 랩 2의 솔루션 노트북이 포함되어 있습니다.\n",
    "* **언어 모델 (PRETRAINED_MODEL_NAME):**<br>\n",
    "문체를 더 잘 구분하려면 다양한 언어 모델을 시도하는 것이 도움이 된다는 사실을 알게 될 수 있습니다.  구체적으로 말해서, 대문자 사용이 중요할 수 있으며 이는 \"대소문자를 구분하는\" 모델을 시도해 볼 수 있음을 의미합니다.\n",
    "* **최대 시퀀스 길이 (MAX_SEQ_LEN):**<br>\n",
    "MAX_SEQ_LENGTH에 사용할 수 있는 값은 64, 128 또는 256입니다.  더 큰 모델(BERT-large, Megatron)에는 메모리 부족 오류를 피하기 위해 더 작은 MAX_SEQ_LENGTH가 필요할 수 있습니다.\n",
    "* **클래스 수 (NUM_CLASSES):**<br>\n",
    "연방주의자 논집의 경우 우리는 Hamilton과 Madison에만 관심을 가졌습니다.  John Jay가 작성한 논문은 데이터세트에서 제외되었습니다.\n",
    "* **배치 크기 (BATCH_SIZE):**<br>\n",
    "배치 크기가 크면 더 빨리 트레이닝할 수 있지만 대규모 언어 모델은 가용 메모리를 빠르게 소모합니다.\n",
    "* **메모리 사용량:**<br>\n",
    "일부 모델은 규모가 매우 큽니다.   트레이닝 도중 \"런타임 오류: CUDA 메모리 부족\"이라는 메시지가 수신되면 배치 크기 또는 시퀀스 길이를 줄이거나 더 작은 언어 모델을 선택하고 커널을 다시 시작한 다음 노트북의 처음부터 다시 시도하십시오.\n",
    "* **정확도 및 손실:**<br>\n",
    "이 프로젝트에서는 분명 95% 이상의 정확도를 달성할 수 있습니다.  모델을 트레이닝할 때 정확도 변화 외에도 손실 값에 주의를 기울이십시오.  손실 값을 매우 작게 줄여서 최상의 결과를 얻을 수 있습니다.\n",
    "* **에포크 수(NUM_EPOCHS):**<br>\n",
    "모델을 위해 더 많은 에포크를 실행해야 할 수도 있습니다(또는 아닐 수도 있음!).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmgJ1YAahMFq"
   },
   "source": [
    "---\n",
    "# 1단계: 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O6FF9nwGhMFr"
   },
   "outputs": [],
   "source": [
    "# Import useful utilities for grading\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_latest_model():  \n",
    "    nemo_model_paths = glob.glob('nemo_experiments/TextClassification/*/checkpoints/*.nemo')\n",
    "    # Sort newest first\n",
    "    nemo_model_paths.sort(reverse=True)\n",
    "    return nemo_model_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNcjqbGYhMFs"
   },
   "source": [
    "데이터는 데이터 디렉토리에 위치하고 있습니다. 다음 셀에서 표시하는 목록을 참고하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rHFJfJSUhMFt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.tsv   test49.tsv  test51.tsv  test53.tsv  test55.tsv  test57.tsv  train.tsv\n",
      "test.tsv  test50.tsv  test52.tsv  test54.tsv  test56.tsv  test62.tsv\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "!ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_JFNwYThMFu"
   },
   "source": [
    "## 데이터 형식 (평가됨)\n",
    "데이터는 NeMo 텍스트 분류에 올바른 형식이 아닙니다. 데이터를 수정하고 DATA_DIR 에 새로운 데이터 세트를 `train_nemo_format.tsv`및 `dev_nemo_format.tsv`로 저장합니다.  테스트 파일을 사용할 필요가 없습니다.  \n",
    "\n",
    "<i><strong style=\"color:green;\">#FIXME</strong></i> 라인을 완성하고 저장된 셀을 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OY79IhPbhMFu"
   },
   "outputs": [],
   "source": [
    "# Correct the format for train.tsv and dev.tsv\n",
    "#   and save the updates in train_nemo_format.tsv and dev_nemo_format.tsv\n",
    "\n",
    "#FIXME train.tsv format\n",
    "#FIXME dev.tsv format\n",
    "!sed 1d $DATA_DIR/train.tsv > $DATA_DIR/train_nemo_format.tsv\n",
    "!sed 1d $DATA_DIR/dev.tsv > $DATA_DIR/dev_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8wjEE5HthMFv",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "train_nemo_format.tsv sample\n",
      "*****\n",
      "Concerning Dangers from Dissensions Between the States For the Independent Journal .To the People of the State of New York : THE three last numbers of this paper have been dedicated to an enumeration of the dangers to which we should be exposed , in a state of disunion , from the arms and arts of foreign nations .I shall now proceed to delineate dangers of a different and , perhaps , still more alarming kind -- those which will in all probability flow from dissensions between the States themselves , and from domestic factions and convulsions .These have been already in some instances slightly anticipated ; but they deserve a more particular and more full investigation .A man must be far gone in Utopian speculations who can seriously doubt that , if these States should either be wholly disunited , or only united in partial confederacies , the subdivisions into which they might be thrown would have frequent and violent contests with each other .To presume a want of motives for such contests as an argument against their existence , would be to forget that men are ambitious , vindictive , and rapacious .To look for a continuation of harmony between a number of independent , unconnected sovereignties in the same neighborhood , would be to disregard the uniform course of human events , and to set at defiance the accumulated experience of ages .The causes of hostility among nations are innumerable .\t0\n",
      "There are some which have a general and almost constant operation upon the collective bodies of society .Of this description are the love of power or the desire of pre-eminence and dominion -- the jealousy of power , or the desire of equality and safety .There are others which have a more circumscribed though an equally operative influence within their spheres .Such are the rivalships and competitions of commerce between commercial nations .And there are others , not less numerous than either of the former , which take their origin entirely in private passions ; in the attachments , enmities , interests , hopes , and fears of leading individuals in the communities of which they are members .Men of this class , whether the favorites of a king or of a people , have in too many instances abused the confidence they possessed ; and assuming the pretext of some public motive , have not scrupled to sacrifice the national tranquillity to personal advantage or personal gratification .The celebrated Pericles , in compliance with the resentment of a prostitute,1 at the expense of much of the blood and treasure of his countrymen , attacked , vanquished , and destroyed the city of the SAMNIANS .The same man , stimulated by private pique against the MEGARENSIANS,2 another nation of Greece , or to avoid a prosecution with which he was threatened as an accomplice of a supposed theft of the statuary Phidias,3 or to get rid of the accusations prepared to be brought against him for dissipating the funds of the state in the purchase of popularity,4 or from a combination of all these causes , was the primitive author of that famous and fatal war , distinguished in the Grecian annals by the name of the PELOPONNESIAN war ; which , after various vicissitudes , intermissions , and renewals , terminated in the ruin of the Athenian commonwealth .\t0\n",
      "The ambitious cardinal , who was prime minister to Henry VIII. , permitting his vanity to aspire to the triple crown,5 entertained hopes of succeeding in the acquisition of that splendid prize by the influence of the Emperor Charles V. To secure the favor and interest of this enterprising and powerful monarch , he precipitated England into a war with France , contrary to the plainest dictates of policy , and at the hazard of the safety and independence , as well of the kingdom over which he presided by his counsels , as of Europe in general .For if there ever was a sovereign who bid fair to realize the project of universal monarchy , it was the Emperor Charles V. , of whose intrigues Wolsey was at once the instrument and the dupe .The influence which the bigotry of one female,6 the petulance of another,7 and the cabals of a third,8 had in the contemporary policy , ferments , and pacifications , of a considerable part of Europe , are topics that have been too often descanted upon not to be generally known .To multiply examples of the agency of personal considerations in the production of great national events , either foreign or domestic , according to their direction , would be an unnecessary waste of time .Those who have but a superficial acquaintance with the sources from which they are to be drawn , will themselves recollect a variety of instances ; and those who have a tolerable knowledge of human nature will not stand in need of such lights to form their opinion either of the reality or extent of that agency .\t0\n",
      "\n",
      "\n",
      "*****\n",
      "dev_nemo_format.tsv sample\n",
      "*****\n",
      "There have been , if I may so express it , almost as many popular as royal wars .The cries of the nation and the importunities of their representatives have , upon various occasions , dragged their monarchs into war , or continued them in it , contrary to their inclinations , and sometimes contrary to the real interests of the State .In that memorable struggle for superiority between the rival houses of AUSTRIA and BOURBON , which so long kept Europe in a flame , it is well known that the antipathies of the English against the French , seconding the ambition , or rather the avarice , of a favorite leader,10 protracted the war beyond the limits marked out by sound policy , and for a considerable time in opposition to the views of the court .The wars of these two last-mentioned nations have in a great measure grown out of commercial considerations , -- the desire of supplanting and the fear of being supplanted , either in particular branches of traffic or in the general advantages of trade and navigation .From this summary of what has taken place in other countries , whose situations have borne the nearest resemblance to our own , what reason can we have to confide in those reveries which would seduce us into an expectation of peace and cordiality between the members of the present confederacy , in a state of separation ? Have we not already seen enough of the fallacy and extravagance of those idle theories which have amused us with promises of an exemption from the imperfections , weaknesses and evils incident to society in every shape ?\t0\n",
      "They would , at the same time , be necessitated to strengthen the executive arm of government , in doing which their constitutions would acquire a progressive direction toward monarchy .It is of the nature of war to increase the executive at the expense of the legislative authority .The expedients which have been mentioned would soon give the States or confederacies that made use of them a superiority over their neighbors .Small states , or states of less natural strength , under vigorous governments , and with the assistance of disciplined armies , have often triumphed over large states , or states of greater natural strength , which have been destitute of these advantages .Neither the pride nor the safety of the more important States or confederacies would permit them long to submit to this mortifying and adventitious superiority .They would quickly resort to means similar to those by which it had been effected , to reinstate themselves in their lost pre-eminence .Thus , we should , in a little time , see established in every part of this country the same engines of despotism which have been the scourge of the Old World .This , at least , would be the natural course of things ; and our reasonings will be the more likely to be just , in proportion as they are accommodated to this standard .These are not vague inferences drawn from supposed or speculative defects in a Constitution , the whole power of which is lodged in the hands of a people , or their representatives and delegates , but they are solid conclusions , drawn from the natural and necessary progress of human affairs .\t0\n",
      "Such a point gained from the British government , and which could not be expected without an equivalent in exemptions and immunities in our markets , would be likely to have a correspondent effect on the conduct of other nations , who would not be inclined to see themselves altogether supplanted in our trade .A further resource for influencing the conduct of European nations toward us , in this respect , would arise from the establishment of a federal navy .There can be no doubt that the continuance of the Union under an efficient government would put it in our power , at a period not very distant , to create a navy which , if it could not vie with those of the great maritime powers , would at least be of respectable weight if thrown into the scale of either of two contending parties .This would be more peculiarly the case in relation to operations in the West Indies .A few ships of the line , sent opportunely to the reinforcement of either side , would often be sufficient to decide the fate of a campaign , on the event of which interests of the greatest magnitude were suspended .Our position is , in this respect , a most commanding one .And if to this consideration we add that of the usefulness of supplies from this country , in the prosecution of military operations in the West Indies , it will readily be perceived that a situation so favorable would enable us to bargain with great advantage for commercial privileges .\t0\n"
     ]
    }
   ],
   "source": [
    "# check your work\n",
    "print(\"*****\\ntrain_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $DATA_DIR/train_nemo_format.tsv\n",
    "print(\"\\n\\n*****\\ndev_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $DATA_DIR/dev_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dnU-9XBthMFv"
   },
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "import os.path\n",
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "step1 = []\n",
    "try:\n",
    "    with open(os.path.join(DATA_DIR,'train_nemo_format.tsv')) as f:\n",
    "        content = f.readlines()\n",
    "        step1 += content[:2]\n",
    "    with open(os.path.join(DATA_DIR,'dev_nemo_format.tsv')) as f:\n",
    "        content = f.readlines()\n",
    "        step1 += content[:2]\n",
    "except:\n",
    "    pass\n",
    "                \n",
    "with open(\"my_assessment/step1.json\", \"w\") as outfile: \n",
    "    json.dump(step1, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEZJBUGGhMFw"
   },
   "source": [
    "---\n",
    "# 2단계: 모델 Configuration 준비\n",
    "기본 모델 configuration 과 가용한 언어 모델을 검토합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lMdUdb2shMFw",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemo_path: text_classification_model.nemo\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "classifier_head:\n",
      "  num_output_layers: 2\n",
      "  fc_dropout: 0.1\n",
      "class_labels:\n",
      "  class_labels_file: null\n",
      "dataset:\n",
      "  num_classes: ???\n",
      "  do_lower_case: false\n",
      "  max_seq_length: 256\n",
      "  class_balancing: null\n",
      "  use_cache: false\n",
      "train_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "validation_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "test_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 2.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  weight_decay: 0.01\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "infer_samples:\n",
      "- by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "  for the monster .\n",
      "- director rob marshall went out gunning to make a great one .\n",
      "- uneasy mishmash of styles and genres .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the default model portion of the config file\n",
    "CONFIG_DIR = \"/dli/task/nemo/examples/nlp/text_classification/conf\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ztWDlhk_hMFx",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what BERT-like language models are available\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7P_unX1hMFy"
   },
   "source": [
    "## 파라미터 설정하기 (평가됨)\n",
    "<i><strong style=\"color:green;\">#FIXME</strong></i> 라인을 완성하고 저장된 셀을 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0WE37OZThMFy"
   },
   "outputs": [],
   "source": [
    "# set the values\n",
    "NUM_CLASSES = 2\n",
    "MAX_SEQ_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "PATH_TO_TRAIN_FILE = \"/dli/task/data/federalist_papers_HM/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"/dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\"\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-cased' # change as desired\n",
    "LR = 1e-4 # change as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cfcOUTIyhMFy"
   },
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "with open(\"my_assessment/step2.json\", \"w\") as outfile: \n",
    "    json.dump([MAX_SEQ_LENGTH, NUM_CLASSES, BATCH_SIZE], outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKVk38hXhMFz"
   },
   "source": [
    "---\n",
    "# 3단계: Trainer Configuration 준비\n",
    "기본 trainer 와 exp_manager configuration를 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KLESTU57hMFz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 100\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 32\n",
      "accelerator: ddp\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "num_sanity_val_steps: 0\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "\n",
      "exp_dir: null\n",
      "name: TextClassification\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.trainer))\n",
    "print(OmegaConf.to_yaml(config.exp_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4tOzKGRhMFz"
   },
   "source": [
    "## 파라미터 지정 (평가됨)\n",
    "자동 혼합 정밀도를 FP16 정밀도로 레벨 1로 설정합니다. MAX_EPOCHS to를 적절한 수준 (약 5~20)으로 설정합니다.  <br><i><strong style=\"color:green;\">#FIXME</strong></i> 라인을 완성하고 저장된 셀을 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5oiYHIkXhMF0"
   },
   "outputs": [],
   "source": [
    "# set the values\n",
    "MAX_EPOCHS = 8\n",
    "AMP_LEVEL = 'O1'\n",
    "PRECISION = 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "T-lnvk2QhMF0"
   },
   "outputs": [],
   "source": [
    "# Run to save for assessment - DO NOT CHANGE\n",
    "with open(\"my_assessment/step3.json\", \"w\") as outfile: \n",
    "    json.dump([MAX_EPOCHS, AMP_LEVEL, PRECISION], outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCZJT3_IhMF1",
    "tags": []
   },
   "source": [
    "---\n",
    "# 4단계: 트레이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHahQCm3hMF1"
   },
   "source": [
    "### Trainer 실행하기 (평가됨)\n",
    "<i><strong style=\"color:green;\">#FIXME</strong></i>를 완성하시고 다음 셀에서 트레이닝 및 검증 배치 사이즈, amp 레벨과 정밀도를 확인하세요.  그 다음 트레이닝을 진행 후 저장된 셀을 실행하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "n32XNxN_hMF1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-05-21 08:25:26 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/omegaconf/basecontainer.py:225: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "    Use OmegaConf.to_yaml(cfg)\n",
      "    \n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo I 2022-05-21 08:25:26 text_classification_with_bert:110] \n",
      "    Config Params:\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 8\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "      num_sanity_val_steps: 0\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "    model:\n",
      "      nemo_path: text_classification_model.nemo\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-cased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      classifier_head:\n",
      "        num_output_layers: 2\n",
      "        fc_dropout: 0.1\n",
      "      class_labels:\n",
      "        class_labels_file: null\n",
      "      dataset:\n",
      "        num_classes: 2\n",
      "        do_lower_case: false\n",
      "        max_seq_length: 256\n",
      "        class_balancing: null\n",
      "        use_cache: false\n",
      "      train_ds:\n",
      "        file_path: /dli/task/data/federalist_papers_HM/train_nemo_format.tsv\n",
      "        batch_size: 8\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      validation_ds:\n",
      "        file_path: /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\n",
      "        batch_size: 8\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      test_ds:\n",
      "        file_path: null\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 2.0e-05\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      infer_samples: []\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: TextClassification\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2022-05-21 08:25:26 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/TextClassification/2022-05-21_08-25-26\n",
      "[NeMo I 2022-05-21 08:25:26 exp_manager:563] TensorboardLogger has been set up\n",
      "Lock 140024062030800 acquired on /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 763kB/s]\n",
      "Lock 140024062030800 released on /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307.lock\n",
      "Lock 140024061713856 acquired on /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\n",
      "Downloading: 100%|███████████████████████████| 213k/213k [00:00<00:00, 50.7MB/s]\n",
      "Lock 140024061713856 released on /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\n",
      "Lock 140024061119504 acquired on /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f.lock\n",
      "Downloading: 100%|███████████████████████████| 29.0/29.0 [00:00<00:00, 41.1kB/s]\n",
      "Lock 140024061119504 released on /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f.lock\n",
      "Lock 140024061713664 acquired on /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\n",
      "Downloading: 100%|███████████████████████████| 436k/436k [00:00<00:00, 56.0MB/s]\n",
      "Lock 140024061713664 released on /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:120] Read 502 examples from /dli/task/data/federalist_papers_HM/train_nemo_format.tsv.\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:239] example 0: ['A', 'few', 'armed', 'vessels', ',', 'judiciously', 'stationed', 'at', 'the', 'entrances', 'of', 'our', 'ports', ',', 'might', 'at', 'a', 'small', 'expense', 'be', 'made', 'useful', 'sentinels', 'of', 'the', 'laws', '.And', 'the', 'government', 'having', 'the', 'same', 'interest', 'to', 'provide', 'against', 'violations', 'everywhere', ',', 'the', 'co-operation', 'of', 'its', 'measures', 'in', 'each', 'State', 'would', 'have', 'a', 'powerful', 'tendency', 'to', 'render', 'them', 'effectual', '.Here', 'also', 'we', 'should', 'preserve', 'by', 'Union', ',', 'an', 'advantage', 'which', 'nature', 'holds', 'out', 'to', 'us', ',', 'and', 'which', 'would', 'be', 'relinquished', 'by', 'separation', '.The', 'United', 'States', 'lie', 'at', 'a', 'great', 'distance', 'from', 'Europe', ',', 'and', 'at', 'a', 'considerable', 'distance', 'from', 'all', 'other', 'places', 'with', 'which', 'they', 'would', 'have', 'extensive', 'connections', 'of', 'foreign', 'trade', '.The', 'passage', 'from', 'them', 'to', 'us', ',', 'in', 'a', 'few', 'hours', ',', 'or', 'in', 'a', 'single', 'night', ',', 'as', 'between', 'the', 'coasts', 'of', 'France', 'and', 'Britain', ',', 'and', 'of', 'other', 'neighboring', 'nations', ',', 'would', 'be', 'impracticable', '.This', 'is', 'a', 'prodigious', 'security', 'against', 'a', 'direct', 'contraband', 'with', 'foreign', 'countries', ';', 'but', 'a', 'circuitous', 'contraband', 'to', 'one', 'State', ',', 'through', 'the', 'medium', 'of', 'another', ',', 'would', 'be', 'both', 'easy', 'and', 'safe', '.The', 'difference', 'between', 'a', 'direct', 'importation', 'from', 'abroad', ',', 'and', 'an', 'indirect', 'importation', 'through', 'the', 'channel', 'of', 'a', 'neighboring', 'State', ',', 'in', 'small', 'parcels', ',', 'according', 'to', 'time', 'and', 'opportunity', ',', 'with', 'the', 'additional', 'facilities', 'of', 'inland', 'communication', ',', 'must', 'be', 'palpable', 'to', 'every', 'man', 'of', 'discernment', '.It', 'is', 'therefore', 'evident', ',', 'that', 'one', 'national', 'government', 'would', 'be', 'able', ',', 'at', 'much', 'less', 'expense', ',', 'to', 'extend', 'the', 'duties', 'on', 'imports', ',', 'beyond', 'comparison', ',', 'further', 'than', 'would', 'be', 'practicable', 'to', 'the', 'States', 'separately', ',', 'or', 'to', 'any', 'partial', 'confederacies', '.']\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:240] subtokens: [CLS] A few armed vessels , j ##udi ##cious ##ly stationed at the entrances of our ports , might at a small expense be made useful sent ##ine ##ls of the laws . And the government having the same interest to provide against violations everywhere , the co - operation of its measures in each State would have a powerful tendency to render them effect ##ual . Here also we should preserve by Union , an advantage which nature holds out to us , and which would be re ##lin ##quished by separation . The United States lie at a great distance from Europe , and at a considerable distance from all other places with which they would have extensive connections of foreign trade . The passage from them to us , in a few hours , or in a single night , as between the coasts of France and Britain , and of other neighboring nations , would be imp ##rac ##tica ##ble . This is a pro ##di ##gio ##us security against a direct con ##tra ##band with foreign countries ; but a circuit ##ous con ##tra ##band to one State , through the medium of another , would be both easy and safe . The difference between a direct import ##ation from abroad , and an indirect import ##ation through the channel of a neighboring State , in small parcel ##s , according to time and opportunity , with the additional facilities of inland communication , must be p ##al ##pable to every [SEP]\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:241] input_ids: 101 138 1374 4223 5956 117 179 17294 9589 1193 8651 1120 1103 19914 1104 1412 9267 117 1547 1120 170 1353 11013 1129 1189 5616 1850 2042 3447 1104 1103 3892 119 1262 1103 1433 1515 1103 1269 2199 1106 2194 1222 13957 7244 117 1103 1884 118 2805 1104 1157 5252 1107 1296 1426 1156 1138 170 3110 12034 1106 19566 1172 2629 4746 119 3446 1145 1195 1431 8333 1118 1913 117 1126 4316 1134 2731 3486 1149 1106 1366 117 1105 1134 1156 1129 1231 2836 24552 1118 8865 119 1109 1244 1311 4277 1120 170 1632 2462 1121 1980 117 1105 1120 170 5602 2462 1121 1155 1168 2844 1114 1134 1152 1156 1138 4154 6984 1104 2880 2597 119 1109 5885 1121 1172 1106 1366 117 1107 170 1374 2005 117 1137 1107 170 1423 1480 117 1112 1206 1103 23278 1104 1699 1105 2855 117 1105 1104 1168 8480 6015 117 1156 1129 24034 19366 11761 2165 119 1188 1110 170 5250 3309 10712 1361 2699 1222 170 2904 14255 4487 10198 1114 2880 2182 132 1133 170 6090 2285 14255 4487 10198 1106 1141 1426 117 1194 1103 5143 1104 1330 117 1156 1129 1241 3123 1105 2914 119 1109 3719 1206 170 2904 13757 1891 1121 6629 117 1105 1126 16673 13757 1891 1194 1103 3094 1104 170 8480 1426 117 1107 1353 24126 1116 117 2452 1106 1159 1105 3767 117 1114 1103 2509 3380 1104 11054 4909 117 1538 1129 185 1348 20786 1106 1451 102\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:239] example 1: ['The', 'suffering', 'States', 'would', 'not', 'long', 'consent', 'to', 'remain', 'associated', 'upon', 'a', 'principle', 'which', 'distributes', 'the', 'public', 'burdens', 'with', 'so', 'unequal', 'a', 'hand', ',', 'and', 'which', 'was', 'calculated', 'to', 'impoverish', 'and', 'oppress', 'the', 'citizens', 'of', 'some', 'States', ',', 'while', 'those', 'of', 'others', 'would', 'scarcely', 'be', 'conscious', 'of', 'the', 'small', 'proportion', 'of', 'the', 'weight', 'they', 'were', 'required', 'to', 'sustain', '.This', ',', 'however', ',', 'is', 'an', 'evil', 'inseparable', 'from', 'the', 'principle', 'of', 'quotas', 'and', 'requisitions', '.There', 'is', 'no', 'method', 'of', 'steering', 'clear', 'of', 'this', 'inconvenience', ',', 'but', 'by', 'authorizing', 'the', 'national', 'government', 'to', 'raise', 'its', 'own', 'revenues', 'in', 'its', 'own', 'way', '.Imposts', ',', 'excises', ',', 'and', ',', 'in', 'general', ',', 'all', 'duties', 'upon', 'articles', 'of', 'consumption', ',', 'may', 'be', 'compared', 'to', 'a', 'fluid', ',', 'which', 'will', ',', 'in', 'time', ',', 'find', 'its', 'level', 'with', 'the', 'means', 'of', 'paying', 'them', '.The', 'amount', 'to', 'be', 'contributed', 'by', 'each', 'citizen', 'will', 'in', 'a', 'degree', 'be', 'at', 'his', 'own', 'option', ',', 'and', 'can', 'be', 'regulated', 'by', 'an', 'attention', 'to', 'his', 'resources', '.The', 'rich', 'may', 'be', 'extravagant', ',', 'the', 'poor', 'can', 'be', 'frugal', ';', 'and', 'private', 'oppression', 'may', 'always', 'be', 'avoided', 'by', 'a', 'judicious', 'selection', 'of', 'objects', 'proper', 'for', 'such', 'impositions', '.If', 'inequalities', 'should', 'arise', 'in', 'some', 'States', 'from', 'duties', 'on', 'particular', 'objects', ',', 'these', 'will', ',', 'in', 'all', 'probability', ',', 'be', 'counterbalanced', 'by', 'proportional', 'inequalities', 'in', 'other', 'States', ',', 'from', 'the', 'duties', 'on', 'other', 'objects', '.In', 'the', 'course', 'of', 'time', 'and', 'things', ',', 'an', 'equilibrium', ',', 'as', 'far', 'as', 'it', 'is', 'attainable', 'in', 'so', 'complicated', 'a', 'subject', ',', 'will', 'be', 'established', 'everywhere', '.']\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:240] subtokens: [CLS] The suffering States would not long consent to remain associated upon a principle which distribute ##s the public burden ##s with so une ##qua ##l a hand , and which was calculated to imp ##over ##ish and op ##press the citizens of some States , while those of others would scarcely be conscious of the small proportion of the weight they were required to sustain . This , however , is an evil ins ##ep ##ara ##ble from the principle of quota ##s and re ##quisition ##s . There is no method of steering clear of this in ##con ##ven ##ience , but by author ##izing the national government to raise its own revenues in its own way . I ##mpo ##sts , ex ##cise ##s , and , in general , all duties upon articles of consumption , may be compared to a fluid , which will , in time , find its level with the means of paying them . The amount to be contributed by each citizen will in a degree be at his own option , and can be regulated by an attention to his resources . The rich may be extra ##va ##gant , the poor can be f ##rug ##al ; and private oppression may always be avoided by a j ##udi ##cious selection of objects proper for such imp ##os ##ition ##s . If in ##e ##qua ##lities should arise in some States from duties on particular objects , these will , in all probability , be counter [SEP]\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:241] input_ids: 101 1109 5601 1311 1156 1136 1263 9635 1106 3118 2628 1852 170 6708 1134 17114 1116 1103 1470 11904 1116 1114 1177 25731 13284 1233 170 1289 117 1105 1134 1108 10056 1106 24034 5909 2944 1105 11769 11135 1103 4037 1104 1199 1311 117 1229 1343 1104 1639 1156 22464 1129 9701 1104 1103 1353 10807 1104 1103 2841 1152 1127 2320 1106 16974 119 1188 117 1649 117 1110 1126 4719 22233 8043 4626 2165 1121 1103 6708 1104 23690 1116 1105 1231 18540 1116 119 1247 1110 1185 3442 1104 10217 2330 1104 1142 1107 7235 7912 16457 117 1133 1118 2351 4404 1103 1569 1433 1106 4693 1157 1319 13081 1107 1157 1319 1236 119 146 24729 10047 117 4252 14636 1116 117 1105 117 1107 1704 117 1155 5078 1852 4237 1104 8160 117 1336 1129 3402 1106 170 8240 117 1134 1209 117 1107 1159 117 1525 1157 1634 1114 1103 2086 1104 6573 1172 119 1109 2971 1106 1129 4415 1118 1296 7888 1209 1107 170 2178 1129 1120 1117 1319 5146 117 1105 1169 1129 12521 1118 1126 2209 1106 1117 3979 119 1109 3987 1336 1129 3908 2497 24581 117 1103 2869 1169 1129 175 27744 1348 132 1105 2029 23309 1336 1579 1129 9226 1118 170 179 17294 9589 4557 1104 4546 4778 1111 1216 24034 2155 8934 1116 119 1409 1107 1162 13284 16652 1431 14368 1107 1199 1311 1121 5078 1113 2440 4546 117 1292 1209 117 1107 1155 9750 117 1129 4073 102\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-05-21 08:25:27 text_classification_dataset:244] label: 0\n",
      "[NeMo W 2022-05-21 08:25:33 text_classification_dataset:250] Found 502 out of 502 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2022-05-21 08:25:33 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-05-21 08:25:33 data_preprocessing:301] Min: 257 |                  Max: 257 |                  Mean: 257.0 |                  Median: 257.0\n",
      "[NeMo I 2022-05-21 08:25:33 data_preprocessing:307] 75 percentile: 257.00\n",
      "[NeMo I 2022-05-21 08:25:33 data_preprocessing:308] 99 percentile: 257.00\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:120] Read 115 examples from /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv.\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:239] example 0: ['There', 'have', 'been', ',', 'if', 'I', 'may', 'so', 'express', 'it', ',', 'almost', 'as', 'many', 'popular', 'as', 'royal', 'wars', '.The', 'cries', 'of', 'the', 'nation', 'and', 'the', 'importunities', 'of', 'their', 'representatives', 'have', ',', 'upon', 'various', 'occasions', ',', 'dragged', 'their', 'monarchs', 'into', 'war', ',', 'or', 'continued', 'them', 'in', 'it', ',', 'contrary', 'to', 'their', 'inclinations', ',', 'and', 'sometimes', 'contrary', 'to', 'the', 'real', 'interests', 'of', 'the', 'State', '.In', 'that', 'memorable', 'struggle', 'for', 'superiority', 'between', 'the', 'rival', 'houses', 'of', 'AUSTRIA', 'and', 'BOURBON', ',', 'which', 'so', 'long', 'kept', 'Europe', 'in', 'a', 'flame', ',', 'it', 'is', 'well', 'known', 'that', 'the', 'antipathies', 'of', 'the', 'English', 'against', 'the', 'French', ',', 'seconding', 'the', 'ambition', ',', 'or', 'rather', 'the', 'avarice', ',', 'of', 'a', 'favorite', 'leader,10', 'protracted', 'the', 'war', 'beyond', 'the', 'limits', 'marked', 'out', 'by', 'sound', 'policy', ',', 'and', 'for', 'a', 'considerable', 'time', 'in', 'opposition', 'to', 'the', 'views', 'of', 'the', 'court', '.The', 'wars', 'of', 'these', 'two', 'last-mentioned', 'nations', 'have', 'in', 'a', 'great', 'measure', 'grown', 'out', 'of', 'commercial', 'considerations', ',', '--', 'the', 'desire', 'of', 'supplanting', 'and', 'the', 'fear', 'of', 'being', 'supplanted', ',', 'either', 'in', 'particular', 'branches', 'of', 'traffic', 'or', 'in', 'the', 'general', 'advantages', 'of', 'trade', 'and', 'navigation', '.From', 'this', 'summary', 'of', 'what', 'has', 'taken', 'place', 'in', 'other', 'countries', ',', 'whose', 'situations', 'have', 'borne', 'the', 'nearest', 'resemblance', 'to', 'our', 'own', ',', 'what', 'reason', 'can', 'we', 'have', 'to', 'confide', 'in', 'those', 'reveries', 'which', 'would', 'seduce', 'us', 'into', 'an', 'expectation', 'of', 'peace', 'and', 'cordiality', 'between', 'the', 'members', 'of', 'the', 'present', 'confederacy', ',', 'in', 'a', 'state', 'of', 'separation', '?', 'Have', 'we', 'not', 'already', 'seen', 'enough', 'of', 'the', 'fallacy', 'and', 'extravagance', 'of', 'those', 'idle', 'theories', 'which', 'have', 'amused', 'us', 'with', 'promises', 'of', 'an', 'exemption', 'from', 'the', 'imperfections', ',', 'weaknesses', 'and', 'evils', 'incident', 'to', 'society', 'in', 'every', 'shape', '?']\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:240] subtokens: [CLS] There have been , if I may so express it , almost as many popular as royal wars . The cries of the nation and the import ##uni ##ties of their representatives have , upon various occasions , dragged their monarch ##s into war , or continued them in it , contrary to their inclination ##s , and sometimes contrary to the real interests of the State . In that memorable struggle for superiority between the rival houses of AU ##ST ##RI ##A and B ##O ##UR ##BO ##N , which so long kept Europe in a flame , it is well known that the anti ##path ##ies of the English against the French , second ##ing the ambition , or rather the a ##var ##ice , of a favorite leader , 10 pro ##tracted the war beyond the limits marked out by sound policy , and for a considerable time in opposition to the views of the court . The wars of these two last - mentioned nations have in a great measure grown out of commercial considerations , - - the desire of su ##pp ##lant ##ing and the fear of being su ##pp ##lanted , either in particular branches of traffic or in the general advantages of trade and navigation . From this summary of what has taken place in other countries , whose situations have borne the nearest resemblance to our own , what reason can we have to con ##fi ##de in those re ##ver ##ies which would seduce us [SEP]\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:241] input_ids: 101 1247 1138 1151 117 1191 146 1336 1177 6848 1122 117 1593 1112 1242 1927 1112 4276 8755 119 1109 13881 1104 1103 3790 1105 1103 13757 19782 4338 1104 1147 6683 1138 117 1852 1672 6070 117 7509 1147 14390 1116 1154 1594 117 1137 1598 1172 1107 1122 117 11565 1106 1147 24753 1116 117 1105 2121 11565 1106 1103 1842 4740 1104 1103 1426 119 1130 1115 14113 5637 1111 21378 1206 1103 6048 2725 1104 21646 9272 20595 1592 1105 139 2346 19556 23904 2249 117 1134 1177 1263 2023 1980 1107 170 9925 117 1122 1110 1218 1227 1115 1103 2848 16606 1905 1104 1103 1483 1222 1103 1497 117 1248 1158 1103 19034 117 1137 1897 1103 170 8997 4396 117 1104 170 5095 2301 117 1275 5250 16550 1103 1594 2894 1103 6263 3597 1149 1118 1839 2818 117 1105 1111 170 5602 1159 1107 4078 1106 1103 4696 1104 1103 2175 119 1109 8755 1104 1292 1160 1314 118 3025 6015 1138 1107 170 1632 4929 4215 1149 1104 2595 19069 117 118 118 1103 4232 1104 28117 8661 9180 1158 1105 1103 2945 1104 1217 28117 8661 16999 117 1719 1107 2440 5020 1104 3404 1137 1107 1103 1704 13300 1104 2597 1105 11167 119 1622 1142 14940 1104 1184 1144 1678 1282 1107 1168 2182 117 2133 7832 1138 16908 1103 6830 14634 1106 1412 1319 117 1184 2255 1169 1195 1138 1106 14255 8702 2007 1107 1343 1231 4121 1905 1134 1156 26317 1366 102\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:239] example 1: ['They', 'would', ',', 'at', 'the', 'same', 'time', ',', 'be', 'necessitated', 'to', 'strengthen', 'the', 'executive', 'arm', 'of', 'government', ',', 'in', 'doing', 'which', 'their', 'constitutions', 'would', 'acquire', 'a', 'progressive', 'direction', 'toward', 'monarchy', '.It', 'is', 'of', 'the', 'nature', 'of', 'war', 'to', 'increase', 'the', 'executive', 'at', 'the', 'expense', 'of', 'the', 'legislative', 'authority', '.The', 'expedients', 'which', 'have', 'been', 'mentioned', 'would', 'soon', 'give', 'the', 'States', 'or', 'confederacies', 'that', 'made', 'use', 'of', 'them', 'a', 'superiority', 'over', 'their', 'neighbors', '.Small', 'states', ',', 'or', 'states', 'of', 'less', 'natural', 'strength', ',', 'under', 'vigorous', 'governments', ',', 'and', 'with', 'the', 'assistance', 'of', 'disciplined', 'armies', ',', 'have', 'often', 'triumphed', 'over', 'large', 'states', ',', 'or', 'states', 'of', 'greater', 'natural', 'strength', ',', 'which', 'have', 'been', 'destitute', 'of', 'these', 'advantages', '.Neither', 'the', 'pride', 'nor', 'the', 'safety', 'of', 'the', 'more', 'important', 'States', 'or', 'confederacies', 'would', 'permit', 'them', 'long', 'to', 'submit', 'to', 'this', 'mortifying', 'and', 'adventitious', 'superiority', '.They', 'would', 'quickly', 'resort', 'to', 'means', 'similar', 'to', 'those', 'by', 'which', 'it', 'had', 'been', 'effected', ',', 'to', 'reinstate', 'themselves', 'in', 'their', 'lost', 'pre-eminence', '.Thus', ',', 'we', 'should', ',', 'in', 'a', 'little', 'time', ',', 'see', 'established', 'in', 'every', 'part', 'of', 'this', 'country', 'the', 'same', 'engines', 'of', 'despotism', 'which', 'have', 'been', 'the', 'scourge', 'of', 'the', 'Old', 'World', '.This', ',', 'at', 'least', ',', 'would', 'be', 'the', 'natural', 'course', 'of', 'things', ';', 'and', 'our', 'reasonings', 'will', 'be', 'the', 'more', 'likely', 'to', 'be', 'just', ',', 'in', 'proportion', 'as', 'they', 'are', 'accommodated', 'to', 'this', 'standard', '.These', 'are', 'not', 'vague', 'inferences', 'drawn', 'from', 'supposed', 'or', 'speculative', 'defects', 'in', 'a', 'Constitution', ',', 'the', 'whole', 'power', 'of', 'which', 'is', 'lodged', 'in', 'the', 'hands', 'of', 'a', 'people', ',', 'or', 'their', 'representatives', 'and', 'delegates', ',', 'but', 'they', 'are', 'solid', 'conclusions', ',', 'drawn', 'from', 'the', 'natural', 'and', 'necessary', 'progress', 'of', 'human', 'affairs', '.']\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:240] subtokens: [CLS] They would , at the same time , be ne ##cess ##itated to strengthen the executive arm of government , in doing which their constitution ##s would acquire a progressive direction toward monarchy . It is of the nature of war to increase the executive at the expense of the legislative authority . The ex ##ped ##ients which have been mentioned would soon give the States or con ##fe ##der ##ac ##ies that made use of them a superiority over their neighbors . Small states , or states of less natural strength , under vigorous governments , and with the assistance of discipline ##d armies , have often triumph ##ed over large states , or states of greater natural strength , which have been des ##ti ##tu ##te of these advantages . Neither the pride nor the safety of the more important States or con ##fe ##der ##ac ##ies would permit them long to submit to this m ##ort ##ifying and advent ##iti ##ous superiority . They would quickly resort to means similar to those by which it had been effect ##ed , to reins ##tate themselves in their lost pre - em ##inen ##ce . Thus , we should , in a little time , see established in every part of this country the same engines of des ##pot ##ism which have been the s ##co ##urge of the Old World . This , at least , would be the natural course of things ; and our reasoning ##s will be the more likely [SEP]\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:241] input_ids: 101 1220 1156 117 1120 1103 1269 1159 117 1129 24928 22371 13512 1106 13346 1103 3275 1981 1104 1433 117 1107 1833 1134 1147 7119 1116 1156 9703 170 8706 2447 1755 14358 119 1135 1110 1104 1103 2731 1104 1594 1106 2773 1103 3275 1120 1103 11013 1104 1103 7663 3748 119 1109 4252 3537 24767 1134 1138 1151 3025 1156 1770 1660 1103 1311 1137 14255 8124 2692 7409 1905 1115 1189 1329 1104 1172 170 21378 1166 1147 11209 119 6844 2231 117 1137 2231 1104 1750 2379 3220 117 1223 24739 6670 117 1105 1114 1103 5052 1104 9360 1181 9099 117 1138 1510 14558 1174 1166 1415 2231 117 1137 2231 1104 3407 2379 3220 117 1134 1138 1151 3532 3121 7926 1566 1104 1292 13300 119 8853 1103 8188 4040 1103 3429 1104 1103 1167 1696 1311 1137 14255 8124 2692 7409 1905 1156 9154 1172 1263 1106 12295 1106 1142 182 12148 8985 1105 16889 17030 2285 21378 119 1220 1156 1976 8037 1106 2086 1861 1106 1343 1118 1134 1122 1125 1151 2629 1174 117 1106 21141 10237 2310 1107 1147 1575 3073 118 9712 19865 2093 119 4516 117 1195 1431 117 1107 170 1376 1159 117 1267 1628 1107 1451 1226 1104 1142 1583 1103 1269 4540 1104 3532 11439 1863 1134 1138 1151 1103 188 2528 27793 1104 1103 2476 1291 119 1188 117 1120 1655 117 1156 1129 1103 2379 1736 1104 1614 132 1105 1412 14417 1116 1209 1129 1103 1167 2620 102\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2022-05-21 08:25:34 text_classification_dataset:244] label: 0\n",
      "[NeMo W 2022-05-21 08:25:35 text_classification_dataset:250] Found 115 out of 115 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2022-05-21 08:25:35 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-05-21 08:25:35 data_preprocessing:301] Min: 257 |                  Max: 257 |                  Mean: 257.0 |                  Median: 257.0\n",
      "[NeMo I 2022-05-21 08:25:35 data_preprocessing:307] 75 percentile: 257.00\n",
      "[NeMo I 2022-05-21 08:25:35 data_preprocessing:308] 99 percentile: 257.00\n",
      "[NeMo I 2022-05-21 08:25:35 text_classification_model:216] Dataloader config or file_path for the test is missing, so no data loader for test is created!\n",
      "[NeMo W 2022-05-21 08:25:35 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "[NeMo W 2022-05-21 08:25:35 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "Lock 140024062516912 acquired on /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda.lock\n",
      "Downloading: 100%|███████████████████████████| 436M/436M [00:07<00:00, 60.5MB/s]\n",
      "Lock 140024062516912 released on /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda.lock\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertEncoder: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-05-21 08:25:45 text_classification_with_bert:118] ===========================================================================================\n",
      "[NeMo I 2022-05-21 08:25:45 text_classification_with_bert:119] Starting training...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2022-05-21 08:25:45 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 2e-05\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2022-05-21 08:25:45 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f59e4982bb0>\" \n",
      "    will be used during training (effective maximum steps = 504) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 504\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss     | 0     \n",
      "1 | bert_model            | BertEncoder          | 108 M \n",
      "2 | classifier            | SequenceClassifier   | 592 K \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "435.610   Total estimated model params size (MB)\n",
      "Epoch 0:  82%|▊| 64/78 [00:06<00:01,  9.40it/s, loss=0.629, v_num=5-26, lr=1.95e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  85%|▊| 66/78 [00:06<00:01,  9.50it/s, loss=0.629, v_num=5-26, lr=1.95e\u001b[A\n",
      "Epoch 0:  90%|▉| 70/78 [00:07<00:00,  9.91it/s, loss=0.629, v_num=5-26, lr=1.95e\u001b[A\n",
      "Epoch 0:  95%|▉| 74/78 [00:07<00:00, 10.30it/s, loss=0.629, v_num=5-26, lr=1.95e\u001b[A\n",
      "Epoch 0: 100%|█| 78/78 [00:07<00:00, 10.68it/s, loss=0.629, v_num=5-26, lr=1.95e\u001b[A[NeMo I 2022-05-21 08:25:55 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 0: 100%|█| 78/78 [00:07<00:00, 10.59it/s, loss=0.629, v_num=5-26, lr=1.95e\n",
      "                                                                                \u001b[AEpoch 0, global step 62: val_loss reached 0.57394 (best 0.57394), saving model to \"/dli/task/nemo_experiments/TextClassification/2022-05-21_08-25-26/checkpoints/TextClassification--val_loss=0.57-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  82%|▊| 64/78 [00:06<00:01, 10.05it/s, loss=0.549, v_num=5-26, lr=1.67e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  87%|▊| 68/78 [00:06<00:00, 10.36it/s, loss=0.549, v_num=5-26, lr=1.67e\u001b[A\n",
      "Epoch 1:  92%|▉| 72/78 [00:06<00:00, 10.78it/s, loss=0.549, v_num=5-26, lr=1.67e\u001b[A\n",
      "Epoch 1:  97%|▉| 76/78 [00:06<00:00, 11.17it/s, loss=0.549, v_num=5-26, lr=1.67e\u001b[A\n",
      "Validating:  87%|██████████████████████████▊    | 13/15 [00:00<00:00, 30.50it/s]\u001b[A[NeMo I 2022-05-21 08:26:08 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 1: 100%|█| 78/78 [00:06<00:00, 11.27it/s, loss=0.549, v_num=5-26, lr=1.67e\n",
      "                                                                                \u001b[AEpoch 1, global step 125: val_loss reached 0.56635 (best 0.56635), saving model to \"/dli/task/nemo_experiments/TextClassification/2022-05-21_08-25-26/checkpoints/TextClassification--val_loss=0.57-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  82%|▊| 64/78 [00:06<00:01, 10.08it/s, loss=0.519, v_num=5-26, lr=1.4e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  87%|▊| 68/78 [00:06<00:00, 10.35it/s, loss=0.519, v_num=5-26, lr=1.4e-\u001b[A\n",
      "Epoch 2:  92%|▉| 72/78 [00:06<00:00, 10.77it/s, loss=0.519, v_num=5-26, lr=1.4e-\u001b[A\n",
      "Epoch 2:  97%|▉| 76/78 [00:06<00:00, 11.16it/s, loss=0.519, v_num=5-26, lr=1.4e-\u001b[A\n",
      "Validating:  87%|██████████████████████████▊    | 13/15 [00:00<00:00, 29.90it/s]\u001b[A[NeMo I 2022-05-21 08:26:20 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 2: 100%|█| 78/78 [00:06<00:00, 11.26it/s, loss=0.519, v_num=5-26, lr=1.39e\n",
      "                                                                                \u001b[AEpoch 2, global step 188: val_loss reached 0.53878 (best 0.53878), saving model to \"/dli/task/nemo_experiments/TextClassification/2022-05-21_08-25-26/checkpoints/TextClassification--val_loss=0.54-epoch=2.ckpt\" as top 3\n",
      "Epoch 3:  82%|▊| 64/78 [00:06<00:01,  9.63it/s, loss=0.478, v_num=5-26, lr=1.12e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  87%|▊| 68/78 [00:06<00:01,  9.89it/s, loss=0.478, v_num=5-26, lr=1.12e\u001b[A\n",
      "Epoch 3:  92%|▉| 72/78 [00:06<00:00, 10.30it/s, loss=0.478, v_num=5-26, lr=1.12e\u001b[A\n",
      "Epoch 3:  97%|▉| 76/78 [00:07<00:00, 10.69it/s, loss=0.478, v_num=5-26, lr=1.12e\u001b[A\n",
      "Validating:  87%|██████████████████████████▊    | 13/15 [00:00<00:00, 29.56it/s]\u001b[A[NeMo I 2022-05-21 08:26:34 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             86.73      95.51      90.91         89\n",
      "    label_id: 1                                             76.47      50.00      60.47         26\n",
      "    -------------------\n",
      "    micro avg                                               85.22      85.22      85.22        115\n",
      "    macro avg                                               81.60      72.75      75.69        115\n",
      "    weighted avg                                            84.41      85.22      84.03        115\n",
      "    \n",
      "Epoch 3: 100%|█| 78/78 [00:07<00:00, 10.79it/s, loss=0.478, v_num=5-26, lr=1.11e\n",
      "                                                                                \u001b[AEpoch 3, global step 251: val_loss reached 0.36573 (best 0.36573), saving model to \"/dli/task/nemo_experiments/TextClassification/2022-05-21_08-25-26/checkpoints/TextClassification--val_loss=0.37-epoch=3.ckpt\" as top 3\n",
      "Epoch 4:  82%|▊| 64/78 [00:06<00:01, 10.20it/s, loss=0.263, v_num=5-26, lr=8.41e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  87%|▊| 68/78 [00:06<00:00, 10.47it/s, loss=0.263, v_num=5-26, lr=8.41e\u001b[A\n",
      "Epoch 4:  92%|▉| 72/78 [00:06<00:00, 10.88it/s, loss=0.263, v_num=5-26, lr=8.41e\u001b[A\n",
      "Epoch 4:  97%|▉| 76/78 [00:06<00:00, 11.28it/s, loss=0.263, v_num=5-26, lr=8.41e\u001b[A\n",
      "Validating:  87%|██████████████████████████▊    | 13/15 [00:00<00:00, 29.62it/s]\u001b[A[NeMo I 2022-05-21 08:26:47 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             88.89      98.88      93.62         89\n",
      "    label_id: 1                                             93.75      57.69      71.43         26\n",
      "    -------------------\n",
      "    micro avg                                               89.57      89.57      89.57        115\n",
      "    macro avg                                               91.32      78.28      82.52        115\n",
      "    weighted avg                                            89.99      89.57      88.60        115\n",
      "    \n",
      "Epoch 4: 100%|█| 78/78 [00:06<00:00, 11.37it/s, loss=0.263, v_num=5-26, lr=8.37e\n",
      "                                                                                \u001b[AEpoch 4, global step 314: val_loss reached 0.33161 (best 0.33161), saving model to \"/dli/task/nemo_experiments/TextClassification/2022-05-21_08-25-26/checkpoints/TextClassification--val_loss=0.33-epoch=4.ckpt\" as top 3\n",
      "Epoch 5:  82%|▊| 64/78 [00:06<00:01, 10.11it/s, loss=0.205, v_num=5-26, lr=5.64e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  87%|▊| 68/78 [00:06<00:00, 10.40it/s, loss=0.205, v_num=5-26, lr=5.64e\u001b[A\n",
      "Epoch 5:  92%|▉| 72/78 [00:06<00:00, 10.82it/s, loss=0.205, v_num=5-26, lr=5.64e\u001b[A\n",
      "Epoch 5:  97%|▉| 76/78 [00:06<00:00, 11.22it/s, loss=0.205, v_num=5-26, lr=5.64e\u001b[A\n",
      "Validating:  87%|██████████████████████████▊    | 13/15 [00:00<00:00, 30.25it/s]\u001b[A[NeMo I 2022-05-21 08:27:00 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             86.41     100.00      92.71         89\n",
      "    label_id: 1                                            100.00      46.15      63.16         26\n",
      "    -------------------\n",
      "    micro avg                                               87.83      87.83      87.83        115\n",
      "    macro avg                                               93.20      73.08      77.93        115\n",
      "    weighted avg                                            89.48      87.83      86.03        115\n",
      "    \n",
      "Epoch 5: 100%|█| 78/78 [00:06<00:00, 11.32it/s, loss=0.205, v_num=5-26, lr=5.59e\n",
      "                                                                                \u001b[AEpoch 5, global step 377: val_loss reached 0.43599 (best 0.33161), saving model to \"/dli/task/nemo_experiments/TextClassification/2022-05-21_08-25-26/checkpoints/TextClassification--val_loss=0.44-epoch=5.ckpt\" as top 3\n",
      "Epoch 6:  82%|▊| 64/78 [00:06<00:01, 10.06it/s, loss=0.064, v_num=5-26, lr=2.86e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  87%|▊| 68/78 [00:06<00:00, 10.33it/s, loss=0.064, v_num=5-26, lr=2.86e\u001b[A\n",
      "Epoch 6:  92%|▉| 72/78 [00:06<00:00, 10.74it/s, loss=0.064, v_num=5-26, lr=2.86e\u001b[A\n",
      "Epoch 6:  97%|▉| 76/78 [00:06<00:00, 11.14it/s, loss=0.064, v_num=5-26, lr=2.86e\u001b[A\n",
      "Validating:  87%|██████████████████████████▊    | 13/15 [00:00<00:00, 29.74it/s]\u001b[A[NeMo I 2022-05-21 08:27:13 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             94.25      92.13      93.18         89\n",
      "    label_id: 1                                             75.00      80.77      77.78         26\n",
      "    -------------------\n",
      "    micro avg                                               89.57      89.57      89.57        115\n",
      "    macro avg                                               84.63      86.45      85.48        115\n",
      "    weighted avg                                            89.90      89.57      89.70        115\n",
      "    \n",
      "Epoch 6: 100%|█| 78/78 [00:06<00:00, 11.24it/s, loss=0.064, v_num=5-26, lr=2.82e\n",
      "                                                                                \u001b[AEpoch 6, global step 440: val_loss reached 0.29942 (best 0.29942), saving model to \"/dli/task/nemo_experiments/TextClassification/2022-05-21_08-25-26/checkpoints/TextClassification--val_loss=0.30-epoch=6.ckpt\" as top 3\n",
      "Epoch 7:  82%|▊| 64/78 [00:06<00:01, 10.17it/s, loss=0.0344, v_num=5-26, lr=8.81\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  87%|▊| 68/78 [00:06<00:00, 10.46it/s, loss=0.0344, v_num=5-26, lr=8.81\u001b[A\n",
      "Epoch 7:  92%|▉| 72/78 [00:06<00:00, 10.88it/s, loss=0.0344, v_num=5-26, lr=8.81\u001b[A\n",
      "Epoch 7:  97%|▉| 76/78 [00:06<00:00, 11.27it/s, loss=0.0344, v_num=5-26, lr=8.81\u001b[A\n",
      "Validating:  87%|██████████████████████████▊    | 13/15 [00:00<00:00, 30.08it/s]\u001b[A[NeMo I 2022-05-21 08:27:26 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             93.41      95.51      94.44         89\n",
      "    label_id: 1                                             83.33      76.92      80.00         26\n",
      "    -------------------\n",
      "    micro avg                                               91.30      91.30      91.30        115\n",
      "    macro avg                                               88.37      86.21      87.22        115\n",
      "    weighted avg                                            91.13      91.30      91.18        115\n",
      "    \n",
      "Epoch 7: 100%|█| 78/78 [00:06<00:00, 11.37it/s, loss=0.0344, v_num=5-26, lr=4.41\n",
      "                                                                                \u001b[AEpoch 7, global step 503: val_loss reached 0.29968 (best 0.29942), saving model to \"/dli/task/nemo_experiments/TextClassification/2022-05-21_08-25-26/checkpoints/TextClassification--val_loss=0.30-epoch=7.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 7: 100%|█| 78/78 [00:12<00:00,  6.04it/s, loss=0.0344, v_num=5-26, lr=4.41\n",
      "[NeMo W 2022-05-21 08:27:32 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "[NeMo I 2022-05-21 08:27:56 text_classification_with_bert:121] Training finished!\n",
      "[NeMo I 2022-05-21 08:27:56 text_classification_with_bert:122] ===========================================================================================\n",
      "[NeMo I 2022-05-21 08:28:18 text_classification_with_bert:127] Model is saved into `.nemo` file: text_classification_model.nemo\n",
      "CPU times: user 2.72 s, sys: 692 ms, total: 3.41 s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the training script, overriding the config values in the command line\n",
    "TC_DIR = \"/dli/task/nemo/examples/nlp/text_classification\"\n",
    "\n",
    "\n",
    "!python $TC_DIR/text_classification_with_bert.py \\\n",
    "        model.dataset.num_classes=$NUM_CLASSES \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.file_path=$PATH_TO_TRAIN_FILE \\\n",
    "        model.validation_ds.file_path=$PATH_TO_VAL_FILE \\\n",
    "        model.infer_samples=[] \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.precision=$PRECISION \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UJQzX3rPhMF2"
   },
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "cmd_log = os.path.join(os.path.dirname(os.path.dirname(get_latest_model())),'cmd-args.log')\n",
    "lightning_logs = os.path.join(os.path.dirname(os.path.dirname(get_latest_model())),'lightning_logs.txt')\n",
    "\n",
    "with open(cmd_log, \"r\") as f:\n",
    "    cmd = f.read()\n",
    "    cmd_list = cmd.split()\n",
    "with open(\"my_assessment/step4.json\", \"w\") as outfile: \n",
    "    json.dump(cmd_list, outfile) \n",
    "    \n",
    "with open(lightning_logs, \"r\") as f:\n",
    "    log = f.readlines()\n",
    "with open(\"my_assessment/step4_lightning.json\", \"w\") as outfile:\n",
    "    json.dump(log, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UqFWUsxhMF2"
   },
   "source": [
    "---\n",
    "# 5단계: 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1jZfWUqhMF2"
   },
   "source": [
    "### 추론 실행하기 (평가됨)\n",
    "추론 블록을 실행하고 결과를 확인하고 저장합니다. (노트: 여기는 추가로 고칠 부분이 없습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mlnzT13DhMF2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2022-05-21 08:28:33 modelPT:137] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    file_path: /dli/task/data/federalist_papers_HM/train_nemo_format.tsv\n",
      "    batch_size: 8\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2022-05-21 08:28:33 modelPT:144] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    file_path: /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\n",
      "    batch_size: 8\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2022-05-21 08:28:33 modelPT:151] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2022-05-21 08:28:33 modelPT:1198] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2022-05-21 08:28:33 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo W 2022-05-21 08:28:33 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-21 08:28:38 modelPT:434] Model TextClassificationModel was successfully restored from nemo_experiments/TextClassification/2022-05-21_08-25-26/checkpoints/TextClassification.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-05-21 08:28:38 text_classification_dataset:250] Found 7 out of 7 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2022-05-21 08:28:39 text_classification_dataset:250] Found 4 out of 4 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2022-05-21 08:28:39 text_classification_dataset:250] Found 8 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2022-05-21 08:28:40 text_classification_dataset:250] Found 7 out of 7 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2022-05-21 08:28:40 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2022-05-21 08:28:40 text_classification_dataset:250] Found 8 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2022-05-21 08:28:41 text_classification_dataset:250] Found 8 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2022-05-21 08:28:41 text_classification_dataset:250] Found 6 out of 6 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2022-05-21 08:28:41 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2022-05-21 08:28:42 text_classification_dataset:250] Found 22 out of 22 sentences with more than 256 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 0, 1, 0, 0], [0, 0, 1, 1], [1, 0, 0, 1, 1, 1, 1, 0], [0, 1, 1, 0, 1, 0, 1], [0, 0, 1, 1, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 0, 0], [0, 1, 0, 1, 1, 0], [1, 0, 0, 0, 1, 1, 0, 1, 0], [1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Run inference for assessment -  - DO NOT CHANGE\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# Instantiate the model by restoring from the latest .nemo checkpoint\n",
    "model = nemo_nlp.models.TextClassificationModel.restore_from(get_latest_model())\n",
    "\n",
    "# Find the latest model path\n",
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "\n",
    "test_files = [\n",
    "    'test49.tsv',\n",
    "    'test50.tsv',\n",
    "    'test51.tsv',\n",
    "    'test52.tsv',\n",
    "    'test53.tsv',\n",
    "    'test54.tsv', \n",
    "    'test55.tsv',\n",
    "    'test56.tsv',\n",
    "    'test57.tsv',\n",
    "    'test62.tsv',\n",
    "]\n",
    "results = []\n",
    "for test_file in test_files:\n",
    "    # get as list and remove header row\n",
    "    filepath = os.path.join(DATA_DIR, test_file)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    del lines[0]\n",
    "    \n",
    "    results.append(model.classifytext(lines, batch_size = 1, max_seq_length = 256))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "f70jcxRDhMF3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMILTON\n",
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "HAMILTON\n",
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "HAMILTON\n",
      "MADISON\n"
     ]
    }
   ],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "author = []\n",
    "for result in results:\n",
    "    avg_result = sum(result) / len(result)\n",
    "    if avg_result < 0.5:\n",
    "        author.append(\"HAMILTON\")\n",
    "        print(\"HAMILTON\")\n",
    "    else:\n",
    "        author.append(\"MADISON\")\n",
    "        print(\"MADISON\")\n",
    "        \n",
    "with open(\"my_assessment/step5.json\", \"w\") as outfile: \n",
    "    json.dump(author, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISARdk0HhMF3"
   },
   "source": [
    "# 6단계: 평가 제출\n",
    "결과가 어땠습니까?  [위키디피아 문서](https://en.wikipedia.org/wiki/The_Federalist_Papers)에 따르면 과거의 전문가들은 Madison이 논쟁이 되고 있는 모든 논문의 저자라고 믿었지만 최근 분석에서는 일부 공저의 가능성도 제시됩니다.  갖고 있는 도구를 사용해 \"모두 MADISON\"이라는 답을 얻을 수도 있습니다.  원한다면 계속 시도해 봐도 되지만 **특정 결과는 평가에 합격하는 데 필요하지 *않습니다***.\n",
    "\n",
    "코드를 올바르게 작성했고 트레이닝 및 추론이 올바르게 작동하고 있다고 확신한다면 다음과 같이 오토그레이더(autograder)에 프로젝트를 제출할 수 있습니다.\n",
    "\n",
    "1. GPU 시작 페이지로 돌아가서 체크 표시를 클릭해 평가를 실행합니다.\n",
    "\n",
    "<img src=\"../images/assessment_checkmark.png\" width=600>\n",
    "\n",
    "2. 잘 하셨습니다!  합격했으면 합격을 알리는 팝업 창이 표시되고 진도에 점수가 추가됩니다.  그렇지 않은 경우 팝업 창에 피드백이 표시됩니다. \n",
    "\n",
    "<img src=\"../images/assessment_pass_popup.png\" width=600>\n",
    "\n",
    "과정 진도 탭에서 항상 평가 진행 상황을 확인할 수 있습니다.  코딩 평가의 부분 값은 여기에 표시되지 않으며, 0점 또는 70점으로 표시됩니다.  동일한 과정 페이지의 트랜스포머 및 배포에서 질문을 완료하면 최종 인증서를 받을 수 있는 자격이 부여됩니다!\n",
    "\n",
    "<img src=\"../images/progress.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlQ_LnwlhMF3"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assessment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
